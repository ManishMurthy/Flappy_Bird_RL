\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Set path for figures
\graphicspath{{Figures/}{report/figures/}}
    
\begin{document}

\title{Playing Flappy Bird Using Deep Reinforcement Learning: A Deep Q-Network Approach\\}

%\author{\IEEEauthorblockN{Manish Murthy}
%\IEEEauthorblockA{\textit{Department of Computer Science} \\
%\textit{University Name}\\
%City, Country \\
%email@university.edu}
%\and
%\IEEEauthorblockN{Sanjana Mandya Lokesh}
%\IEEEauthorblockA{\textit{Department of Computer Science} \\
%\textit{University Name}\\
%City, Country \\
%email@university.edu}
%}

\maketitle

\begin{abstract}
Flappy Bird is a straightforward yet challenging game where players guide a bird through openings between pipes by making it flap its wings. This paper investigates how a reinforcement learning agent can be trained to play Flappy Bird autonomously using Deep Q-Networks (DQN). We demonstrate that by utilizing a compact state representation and a carefully designed reward function, our agent successfully learns an effective policy through experience. Rather than manually coding specific rules, the agent learns optimal behavior through trial and error, determining when to flap and when to glide to maximize its survival time. We detail our implementation approach, including environment construction, neural network architecture, and the impact of various hyperparameters. Our results show that the DQN agent achieves an average score of 15.7 after 5000 training episodes, significantly outperforming random play and approaching skilled human performance. This work illustrates how reinforcement learning techniques originally developed for Atari games can be effectively applied to other dynamic environments and provides insights into the practical challenges of implementing DQN for real-time decision-making tasks.
\end{abstract}

\begin{IEEEkeywords}
Deep Reinforcement Learning, Deep Q-Networks, Flappy Bird, Game AI, Experience Replay, Epsilon-greedy Policy
\end{IEEEkeywords}

% Include each section
\input{latex_sections/1_introduction}
\input{latex_sections/2_background}
\input{latex_sections/3_methodology}
\input{latex_sections/4_results}
\input{latex_sections/5_challenges}
\input{latex_sections/6_conclusion}

% References
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}