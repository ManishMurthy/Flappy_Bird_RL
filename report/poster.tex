\documentclass[a1paper,portrait]{article}
\usepackage[margin=1.5cm]{geometry}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{tcolorbox}
\usepackage{times}

% Set page to white background
\pagecolor{white}

% Define blue color for headers
\definecolor{headerblue}{RGB}{0,114,189}

% Define box styles
\tcbset{
  enhanced,
  colback=white,
  colframe=headerblue,
  fonttitle=\bfseries\large,
  colbacktitle=headerblue,
  coltitle=white,
  boxed title style={colframe=headerblue},
  boxrule=1pt,
  arc=3mm,
  top=3mm,
  bottom=3mm
}

% Custom section title command
\newcommand{\postersection}[1]{
  \vspace{0.5cm}
  \noindent\textcolor{headerblue}{\Large\textbf{#1}}
  \vspace{0.3cm}
}

% Set up the title
\title{\Huge\textbf{Playing Flappy Bird Using Deep Reinforcement Learning}}
\author{\Large Your Name}
\date{May 7, 2025}

\begin{document}

% Remove page number
\thispagestyle{empty}

% Title section
\begin{center}
\textcolor{headerblue}{\VERYHuge\textbf{Playing Flappy Bird Using}}
\vspace{0.5cm}

\textcolor{headerblue}{\VERYHuge\textbf{Deep Reinforcement Learning}}
\vspace{1cm}

\Large\textbf{Your Name}\\
\large Department of Computer Science\\
\large University Name\\
\large May 7, 2025
\end{center}

\vspace{1cm}

% Main content in 3 columns
\begin{multicols}{3}

\postersection{Abstract}
Flappy Bird is a straightforward yet challenging game where players guide a bird through openings between pipes by making it flap its wings. This research investigates how a reinforcement learning agent can be trained to play Flappy Bird autonomously using Deep Q-Networks (DQN).

We demonstrate that by utilizing a compact state representation and a carefully designed reward function, our agent successfully learns an effective policy through experience. Rather than manually coding specific rules, the agent learns optimal behavior through trial and error.

After 5000 training episodes and evaluation on 100 test episodes, our DQN agent achieves an average score of 15.7, significantly outperforming random play and approaching skilled human performance.

\postersection{Introduction}
Deep reinforcement learning combines reinforcement learning with deep neural networks to enable agents to learn directly from high-dimensional sensory inputs without handcrafted features.

In this research, we apply the Deep Q-Network (DQN) approach to the Flappy Bird game environment:
\begin{itemize}
    \item The agent learns through trial and error
    \item The agent must determine when to flap wings and when to glide
    \item Success requires precise timing and decision-making
    \item The environment has continuous state spaces and physics-based dynamics
\end{itemize}

This straightforward yet challenging environment provides an ideal testbed for evaluating reinforcement learning algorithms in dynamic control tasks.

\postersection{Reinforcement Learning Background}
\textbf{Markov Decision Process (MDP):}
\begin{itemize}
    \item States ($s \in S$): Bird position, velocity, pipe locations
    \item Actions ($a \in A$): Flap or do nothing
    \item Transition function ($P(s'|s,a)$): Game physics
    \item Reward function ($R(s,a,s')$): Points for survival and passing pipes
    \item Discount factor ($\gamma$): Balances immediate vs. future rewards
\end{itemize}

\textbf{Q-Learning:} Learns action-value function $Q(s,a)$ representing expected return when taking action $a$ in state $s$ and following the optimal policy thereafter.

\textbf{Deep Q-Network (DQN):} Uses deep neural networks to approximate the Q-function, enabling learning in environments with large or continuous state spaces.

\postersection{Methodology}
\textbf{Environment Implementation:}
\begin{itemize}
    \item Custom Flappy Bird environment with physics-based bird movement
    \item 30 frames per second simulation
    \item Pipes generated with random heights but constant gap size
    \item Simplified collision detection
\end{itemize}

\textbf{State Representation:} Instead of raw pixels, we use 5 normalized features:
\begin{enumerate}
    \item Bird's vertical position
    \item Bird's vertical velocity
    \item Horizontal distance to next pipe
    \item Height difference to top pipe
    \item Height difference to bottom pipe
\end{enumerate}

\textbf{Reward Structure:}
\begin{itemize}
    \item +0.1 for each frame survived (encouraging longevity)
    \item +1.0 for successfully passing a pipe
    \item -1.0 for collisions with pipes or ground
\end{itemize}

\textbf{Neural Network Architecture:}
\begin{itemize}
    \item Input layer: 5 neurons (state features)
    \item Hidden layers: 64-64-32 neurons with ReLU activation
    \item Output layer: 2 neurons (Q-values for each action)
    \item Dropout rate of 0.2 after first hidden layer
\end{itemize}

\textbf{Training Procedure:}
\begin{itemize}
    \item Experience replay buffer (capacity: 10,000 transitions)
    \item Mini-batch size: 32 experiences
    \item Target network updated every 10 episodes
    \item Epsilon-greedy exploration (ε: 1.0 → 0.01, decay: 0.9995)
    \item Adam optimizer (learning rate: 0.0005)
    \item Discount factor (γ): 0.99
    \item Total training: 5000 episodes
    \item Testing: 100 episodes per evaluation
\end{itemize}

\postersection{Hyperparameter Sensitivity}
We conducted extensive tests on hyperparameter sensitivity, evaluating performance after 5000 training episodes with 100 test episodes for each configuration.

\begin{center}
\textbf{Learning Rate vs. Discount Factor Heatmap}
\vspace{0.5cm}

[HYPERPARAMETER SENSITIVITY HEATMAP]
\end{center}

Optimal performance was achieved with a learning rate of 0.001 and a discount factor of 0.99, demonstrating the sensitivity of the DQN approach to proper hyperparameter tuning.

\postersection{Results}
\textbf{Learning Performance:}
Our DQN agent's performance showed a clear progression through three phases:
\begin{itemize}
    \item Initial exploration (episodes 1-200): Average score 0.4 pipes
    \item Rapid improvement (episodes 200-500): Average score 5.2 pipes
    \item Policy refinement (episodes 500-5000): Final average score 15.7 pipes
\end{itemize}

\textbf{Performance Comparison:}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Agent} & \textbf{Avg. Score} & \textbf{Max Score} \\
\midrule
Random Actions & 0.01 & 1 \\
Rule-based & 4.3 & 11 \\
\textbf{DQN (our approach)} & \textbf{15.7} & \textbf{41} \\
Human Expert & 20+ & 50+ \\
\bottomrule
\end{tabular}
\end{center}

\begin{center}
\textbf{Learning Curve Over 5000 Episodes}
\vspace{0.5cm}

[LEARNING CURVE GRAPH]
\end{center}

\postersection{Catastrophic Forgetting}
When training sequentially on different pipe configurations, we observed performance degradation on previously learned configurations:

\begin{center}
[CATASTROPHIC FORGETTING GRAPH]
\end{center}

This phenomenon, known as catastrophic forgetting, shows how the agent's performance on Task A degrades as it learns Task B and C successively during the 5000 training episodes.

\postersection{Environment Variability}
We tested our trained agent across various pipe configurations after 5000 training episodes:

\begin{center}
\textbf{Performance Across Pipe Configurations}

[ENVIRONMENT VARIABILITY BAR CHART]
\end{center}

The agent's performance decreased as environmental complexity increased, with the most significant drop occurring in the Y-Junction configuration, highlighting the challenges of generalization in reinforcement learning.

\postersection{Policy Visualization}
Below we visualize the agent's learned policy after 5000 training episodes, showing when it decides to flap (blue) vs. do nothing (red) based on the bird's position and pipe gap:

\begin{center}
[POLICY VISUALIZATION HEATMAP]
\end{center}

Note the complex decision boundary that emerged from training, demonstrating how the agent learned to account for physics-based momentum.

\postersection{Conclusion \& Future Work}
Our DQN approach successfully learns to play Flappy Bird autonomously, achieving competitive performance after 5000 training episodes with evaluations on 100 test episodes. The agent learns sophisticated control strategies through experience rather than explicit programming.

\textbf{Key Findings:}
\begin{itemize}
    \item Compact state representation is more efficient than raw pixels
    \item Neural network architecture with 64-64-32 neurons provides optimal balance
    \item Carefully designed reward structure is critical for learning
    \item Performance approaches human-level play but challenges remain
\end{itemize}

\textbf{Future Directions:}
\begin{itemize}
    \item Explore distributional reinforcement learning for better uncertainty modeling
    \item Investigate model-based methods for improved sample efficiency
    \item Apply transformer-based architectures for temporal understanding
    \item Explore multi-objective reinforcement learning to optimize for both score and flight smoothness
\end{itemize}

Our approach demonstrates the effectiveness of deep reinforcement learning for tasks requiring precise timing and control, with broader applications to robotics and real-time control problems.

\end{multicols}

\end{document}