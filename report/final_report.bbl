% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski \emph{et~al.},
  ``Human-level control through deep reinforcement learning,'' \emph{Nature},
  vol. 518, no. 7540, pp. 529--533, 2015.

\bibitem{hafner2023mastering}
D.~Hafner, J.~Pasukonis, J.~Ba, and T.~Lillicrap, ``Mastering diverse domains
  through world models,'' \emph{Nature}, vol. 617, no. 7952, pp. 288--294,
  2023.

\bibitem{dabney2020distributional}
W.~Dabney, M.~Rowland, M.~G. Bellemare, and R.~Munos, ``Distributional
  reinforcement learning with quantile regression,'' in \emph{Proceedings of
  the AAAI Conference on Artificial Intelligence}, vol.~32, 2020.

\bibitem{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto, ``Reinforcement learning: An introduction,''
  \emph{MIT press}, 2018.

\bibitem{kumar2023offline}
A.~Kumar, A.~Zhou, G.~Tucker, and S.~Levine, ``Offline reinforcement learning:
  Fundamental barriers for value function approximation,'' \emph{Advances in
  Neural Information Processing Systems}, vol.~35, pp. 5780--5792, 2023.

\bibitem{silver2017mastering}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L.~Baker, M.~Lai, A.~Bolton \emph{et~al.}, ``Mastering the game of
  go without human knowledge,'' \emph{Nature}, vol. 550, no. 7676, pp.
  354--359, 2017.

\bibitem{badia2020agent57}
A.~P. Badia, B.~Piot, S.~Kapturowski, P.~Sprechmann, A.~Vitvitskyi, Z.~D. Guo,
  and C.~Blundell, ``Agent57: Outperforming the atari human benchmark,''
  \emph{Proceedings of the 37th International Conference on Machine Learning},
  pp. 507--517, 2020.

\bibitem{vinyals2019grandmaster}
O.~Vinyals, I.~Babuschkin, W.~M. Czarnecki, M.~Mathieu, A.~Dudzik, J.~Chung,
  D.~H. Choi, R.~Powell, T.~Ewalds, P.~Georgiev \emph{et~al.}, ``Grandmaster
  level in starcraft ii using multi-agent reinforcement learning,''
  \emph{Nature}, vol. 575, no. 7782, pp. 350--354, 2019.

\bibitem{yu2022planning}
K.~Yu, A.~Kumar, J.~Tompson, S.~Levine, and K.~Hausman, ``Planning with
  diffusion for flexible behavior synthesis,'' \emph{International Conference
  on Machine Learning}, pp. 25\,472--25\,493, 2022.

\bibitem{chen2021decision}
L.~Chen, K.~Lu, A.~Rajeswaran, K.~Lee, A.~Grover, M.~Laskin, P.~Abbeel,
  A.~Srinivas, and I.~Mordatch, ``Decision transformer: Reinforcement learning
  via sequence modeling,'' in \emph{Advances in Neural Information Processing
  Systems}, 2021, pp. 15\,084--15\,097.

\bibitem{lee2022multi}
K.-H. Lee, O.~Nachum, M.~Yang, L.~Lee, D.~Freeman, W.~Xu, S.~Guadarrama,
  I.~Fischer, S.~Levine, C.~Finn \emph{et~al.}, ``Multi-game decision
  transformers,'' \emph{Advances in Neural Information Processing Systems},
  vol.~35, pp. 36\,525--36\,541, 2022.

\bibitem{wang2022offline}
L.~Wang, Q.~Zhang, B.~Kang, J.~Pan, P.~J. Bakker, K.~Zhou, H.~Lee, J.~Luo,
  H.~Wang, Z.~Guo, O.~Guo, W.~Lee, and C.~Lu, ``Offline reinforcement learning
  with implicit q-learning,'' \emph{arXiv preprint arXiv:2110.06169}, 2022.

\bibitem{yang2023foundation}
S.~Yang, A.~Gu, K.~Koishida, S.~Levine, C.~Finn, and S.~G. Shixiang,
  ``Foundation models for decision making: Problems, methods, and
  opportunities,'' \emph{arXiv preprint arXiv:2303.04129}, 2023.

\bibitem{fujimoto2021minimalist}
S.~Fujimoto and S.~S. Gu, ``A minimalist approach to offline reinforcement
  learning,'' \emph{Advances in Neural Information Processing Systems},
  vol.~34, pp. 20\,132--20\,145, 2021.

\bibitem{schulman2023proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov, ``Proximal
  policy optimization algorithms,'' \emph{arXiv preprint arXiv:2005.12729},
  2023.

\end{thebibliography}
